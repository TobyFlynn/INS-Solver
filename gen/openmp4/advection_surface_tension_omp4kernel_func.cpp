//
// auto-generated by op2.py
//

void advection_surface_tension_omp4_kernel(
  double *arg0,
  double *data1,
  int dat1size,
  double *data2,
  int dat2size,
  double *data3,
  int dat3size,
  double *data4,
  int dat4size,
  double *data5,
  int dat5size,
  double *data6,
  int dat6size,
  int count,
  int num_teams,
  int nthread){

  double arg0_l = *arg0;
  #pragma omp target teams num_teams(num_teams) thread_limit(nthread) map(to:data1[0:dat1size],data2[0:dat2size],data3[0:dat3size],data4[0:dat4size],data5[0:dat5size],data6[0:dat6size]) \
    map(to: weber_ompkernel)
  #pragma omp distribute parallel for schedule(static,1)
  for ( int n_op=0; n_op<count; n_op++ ){
    //variable mapping
    const double *alpha = &arg0_l;
    const double *curv = &data1[10*n_op];
    const double *nx = &data2[10*n_op];
    const double *ny = &data3[10*n_op];
    const double *s = &data4[10*n_op];
    double *st_x = &data5[10*n_op];
    double *st_y = &data6[10*n_op];

    //inline function
    
    const double PI = 3.141592653589793238463;
    for(int i = 0; i < 10; i++) {








      double delta = (PI / *alpha) * (1.0 / cosh(PI * s[i] / *alpha)) * (1.0 / cosh(PI * s[i] / *alpha));
      st_x[i] = delta * (curv[i] * nx[i] / weber_ompkernel);
      st_y[i] = delta * (curv[i] * ny[i] / weber_ompkernel);
    }
    //end inline func
  }

  *arg0 = arg0_l;
}
